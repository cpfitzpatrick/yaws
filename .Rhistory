lat_sec<-(1-sec_death-sec_ter)*(1-lat_p)
lat_lat<-1-lat_death-lat_sec
# ter is tertiary stage cases
ter_death<-prob_death
ter_ter<-1-ter_death
#######################run Markov model of effects (no eradication)############################
# run model
for (i in 2:cycle_num) {
new[,i,,]<-new[,i-1,,]*new_new[,i-1,,]+repro_num[,i-1,,]*(1-exit_prob[,i-1,,])^max(i-1-burn,0)*pri[,i-1,,]
pri[,i,,]<-pri[,i-1,,]*pri_pri[,i-1,,]+new_pri[,i-1,,]*new[,i-1,,]
sec[,i,,]<-sec[,i-1,,]*sec_sec[,i-1,,]+pri_sec[,i-1,,]*pri[,i-1,,]+lat_sec[,i-1,,]*lat[,i-1,,]
lat[,i,,]<-lat[,i-1,,]*lat_lat[,i-1,,]+sec_lat[,i-1,,]*sec[,i-1,,]
ter[,i,,]<-ter[,i-1,,]*ter_ter[,i-1,,]+sec_ter[,i-1,,]*sec[,i-1,,]
}
# we collapse for world total (by cycle, mainting uncertainty)
# new
new_collapse<-apply(new,c(2,3,4),sum)
new_collapse_best<-apply(new_collapse,1,function(x) quantile(x, probs=c(0.50),na.rm=TRUE))
plot(new_collapse_best)
# primary
pri_collapse<-apply(pri,c(2,3,4),sum)
pri_collapse_best<-apply(pri_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(pri_collapse_best)
# secondary
sec_collapse<-apply(sec,c(2,3,4),sum)
sec_collapse_best<-apply(sec_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(sec_collapse_best)
# latent
lat_collapse<-apply(lat,c(2,3,4),sum)
lat_collapse_best<-apply(lat_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(lat_collapse_best)
# tertiary
ter_collapse<-apply(ter,c(2,3,4),sum)
ter_collapse_best<-apply(ter_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(ter_collapse_best)
# early=primary and secondary
early<-pri+sec
early_collapse<-pri_collapse+sec_collapse
early_collapse_best<-apply(early_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(early_collapse_best)
#######################run Markov model of effects (eradication)#########################
# programmatic parameters
# these we allow to vary by country and cycle, with no correlation (see "tool box" below to impose correlation)
# coverage
cover<-array4dz
# in first (TCT) round
for (i in 1:ctry_num) {cover[i,(burn+1):cycle_num,,]<-runif(ndunc,min=0.90, max=0.99)}
# therafter in (TTT) round(s)
cover[,(burn+1+1):cycle_num,,]<-1
# eligibility
eligible<-array4dz
# in first (TCT) round
for (i in 1:ctry_num) {eligible[i,(burn+1):cycle_num,,]<-runif(ndunc,min=0.98, max=0.99)}
# therafter in (TTT) round(s)
eligible[,(burn+1+1):cycle_num,,]<-1
# cure
cure<-array4d
for (i in 1:ctry_num) {cure[i,,,]<-runif(ndunc,min=0.96, max=1.00)}
# use same starting numbers as in baseline
new_erad<-new
pri_erad<-pri
sec_erad<-sec
lat_erad<-lat
ter_erad<-ter
# run the model starting from the TCT (total community treatment) round in the first period (after burn-in)
for (i in (burn+1):cycle_num) {
new_erad[,i,,]<-new_erad[,i-1,,]*new_new[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])+repro_num[,i-1,,]*(1-exit_prob[,i-1,,])^max(i-1-burn,0)*pri_erad[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])
pri_erad[,i,,]<-pri_erad[,i-1,,]*pri_pri[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])+new_pri[,i-1,,]*new_erad[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])
sec_erad[,i,,]<-sec_erad[,i-1,,]*sec_sec[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])+pri_sec[,i-1,,]*pri_erad[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])+lat_sec[,i-1,,]*lat_erad[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])
lat_erad[,i,,]<-lat_erad[,i-1,,]*lat_lat[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])+sec_lat[,i-1,,]*sec_erad[,i-1,,]*(1-cover[,i-1,,]*eligible[,i-1,,]*cure[,i-1,,])
ter_erad[,i,,]<-ter_erad[,i-1,,]*ter_ter[,i-1,,]+sec_ter[,i-1,,]*sec_erad[,i-1,,]
}
# collapse
# primary
pri_erad_collapse<-apply(pri_erad,c(2,3,4),sum)
pri_erad_collapse_best<-apply(pri_erad_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(pri_erad_collapse_best)
# secondary
sec_erad_collapse<-apply(sec_erad,c(2,3,4),sum)
sec_erad_collapse_best<-apply(sec_erad_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(sec_erad_collapse_best)
# latent
lat_erad_collapse<-apply(lat_erad,c(2,3,4),sum)
lat_erad_collapse_best<-apply(lat_erad_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(lat_erad_collapse_best)
# tertiary
ter_erad_collapse<-apply(ter_erad,c(2,3,4),sum)
ter_erad_collapse_best<-apply(ter_erad_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(ter_erad_collapse_best)
# early
early_erad<-pri_erad+sec_erad
early_erad_collapse<-pri_erad_collapse+sec_erad_collapse
early_erad_collapse_best<-apply(early_erad_collapse,1,function(x) quantile(x, probs=c(0.5),na.rm=TRUE))
plot(early_erad_collapse_best)
####################calculate incremental effects########################
# DALYs
# based on mean and 95th CI from IHME GBD
dw_early<-get.beta.par(p=c(0.025,0.5,0.975),q=c(0.016,0.029,0.048))
dw_late<-get.beta.par(p=c(0.025,0.5,0.975),q=c(0.271,0.398,0.543))
temp<-rbeta(ndunc,shape1=dw_early[1], shape2=dw_early[2])
early_daly<-array4dz
for (i in 1:ndunc) {early_daly[,,i,]<-temp[i]}
temp<-rbeta(ndunc,shape1=dw_late[1], shape2=dw_late[2])
late_daly<-array4dz
for (i in 1:ndunc) {late_daly[,,i,]<-temp[i]}
# difference in effects between eradication and no eradication
pri_diff<-pri_collapse-pri_erad_collapse
sec_diff<-sec_collapse-sec_erad_collapse
ter_diff<-ter_collapse-ter_erad_collapse
early_diff<-early_collapse-early_erad_collapse
# plot mean differences for visual check
pri_diff_best<-apply(pri_diff,1,mean)
sec_diff_best<-apply(sec_diff,1,mean)
ter_diff_best<-apply(ter_diff,1,mean)
early_diff_best<-apply(early_diff,1,mean)
plot(pri_diff_best[1:cycle_num]/1000, x=1:cycle_num, xlab="", ylab="thousands")
plot(sec_diff_best[1:cycle_num]/1000, x=1:cycle_num, xlab="", ylab="thousands")
plot(ter_diff_best[1:cycle_num]/1000, x=1:cycle_num, xlab="", ylab="thousands")
plot(early_diff_best[1:cycle_num]/1000, x=1:cycle_num, xlab="", ylab="thousands")
# calculate highs and lows for the confidence intervals
early_diff_lo<-apply(early_diff,1,function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
early_diff_hi<-apply(early_diff,1,function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
ter_diff_lo<-apply(ter_diff,1,function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
ter_diff_hi<-apply(ter_diff,1,function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
# for presentation of disease dynamics and impact, collapse into years not cycles
early_diff_best_yearly<-rep(NA, years)
early_diff_lo_yearly<-rep(NA, years)
early_diff_hi_yearly<-rep(NA, years)
ter_diff_best_yearly<-rep(NA, years)
ter_diff_lo_yearly<-rep(NA, years)
ter_diff_hi_yearly<-rep(NA, years)
num=1
for (i in seq(from=burn+1, to=cycle_num, by=2)) {
early_diff_best_yearly[num]<-(early_diff_best[i]+early_diff_best[i+1])/2
early_diff_lo_yearly[num]<-(early_diff_lo[i]+early_diff_lo[i+1])/2
early_diff_hi_yearly[num]<-(early_diff_hi[i]+early_diff_hi[i+1])/2
ter_diff_best_yearly[num]<-(ter_diff_best[i]+ter_diff_best[i+1])/2
ter_diff_lo_yearly[num]<-(ter_diff_lo[i]+ter_diff_lo[i+1])/2
ter_diff_hi_yearly[num]<-(ter_diff_hi[i]+ter_diff_hi[i+1])/2
num=num+1
}
plot(early_diff_best_yearly[1:years]/1000, x=2015:2050, xlab="", ylab="thousands")
p <- ggplot(as.data.frame(cbind(year=c(2015:2050),early_diff_best_yearly,early_diff_lo_yearly,early_diff_hi_yearly)), aes(year,early_diff_best_yearly/1000, ymin=early_diff_lo_yearly/1000,ymax=early_diff_hi_yearly/1000))
p + geom_pointrange() + scale_y_continuous(limits=c(), "thousands")
p + geom_pointrange() + expand_limits(y = 0) + scale_y_continuous(limits=c(), "thousands", expand = c(0, 0)) + theme_tufte() + geom_rangeframe()
plot(ter_diff_best_yearly[1:years]/1000, x=2015:2050, xlab="", ylab="thousands")
p <- ggplot(as.data.frame(cbind(year=c(2015:2050),ter_diff_best_yearly,ter_diff_lo_yearly,ter_diff_hi_yearly)), aes(year,ter_diff_best_yearly/1000, ymin=ter_diff_lo_yearly/1000,ymax=ter_diff_hi_yearly/1000))
p + geom_pointrange() + scale_y_continuous(limits=c(), "thousands") + scale_x_continuous(limits=c(), "year")
p + geom_pointrange() +  expand_limits(y = 0) + scale_y_continuous(limits=c(), "thousands", expand = c(0, 0)) + theme_tufte() + geom_rangeframe()
# discounted effects (no eradication)
early_disc<-early
ter_disc<-ter
early_a_late_disc<-early+ter
for (i in 2:cycle_num) {
early_disc[,i,,]<-early_disc[,i,,]/(1+disc_e[,i,,])^max(i-1-burn,0)
ter_disc[,i,,]<-ter_disc[,i,,]/(1+disc_e[,i,,])^max(i-1-burn,0)
early_a_late_disc[,i,,]<-early_a_late_disc[,i,,]/(1+disc_e[,i,,])^max(i-1-burn,0)
}
early_disc_collapse<-apply(early_disc, c(1,3,4), sum)
ter_disc_collapse<-apply(ter_disc, c(1,3,4), sum)
early_a_late_disc_collapse<-apply(early_a_late_disc, c(1,3,4), sum)
# summary table of (discounted period) effects
# early
early_disc_collapse_t<-abind(early_disc_collapse,apply(early_disc_collapse, c(2,3), sum), along=1)
early_disc_collapse_summary<-as.data.frame(apply(early_disc_collapse_t, 1, mean))
colnames(early_disc_collapse_summary)<-"best"
early_disc_collapse_summary$lo<-apply(early_disc_collapse_t, 1, function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
early_disc_collapse_summary$hi<-apply(early_disc_collapse_t, 1, function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
early_disc_collapse_summary
early_disc_collapse_summary$country<-row.names(early_disc_collapse_summary)
early_disc_collapse_summary<-early_disc_collapse_summary[,c(4,1,2,3)]
early_disc_collapse_summary
# late
ter_disc_collapse_t<-abind(ter_disc_collapse,apply(ter_disc_collapse, c(2,3), sum), along=1)
ter_disc_collapse_summary<-as.data.frame(apply(ter_disc_collapse_t, 1, mean))
colnames(ter_disc_collapse_summary)<-"best"
ter_disc_collapse_summary$lo<-apply(ter_disc_collapse_t, 1, function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
ter_disc_collapse_summary$hi<-apply(ter_disc_collapse_t, 1, function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
ter_disc_collapse_summary
ter_disc_collapse_summary$country<-row.names(ter_disc_collapse_summary)
ter_disc_collapse_summary<-ter_disc_collapse_summary[,c(4,1,2,3)]
ter_disc_collapse_summary
# early and late
early_a_late_disc_collapse_t<-abind(early_a_late_disc_collapse,apply(early_a_late_disc_collapse, c(2,3), sum), along=1)
early_a_late_disc_collapse_summary<-as.data.frame(apply(early_a_late_disc_collapse_t, 1, mean))
colnames(early_a_late_disc_collapse_summary)<-"best"
early_a_late_disc_collapse_summary$lo<-apply(early_a_late_disc_collapse_t, 1, function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
early_a_late_disc_collapse_summary$hi<-apply(early_a_late_disc_collapse_t, 1, function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
early_a_late_disc_collapse_summary
early_a_late_disc_collapse_summary$country<-row.names(early_a_late_disc_collapse_summary)
early_a_late_disc_collapse_summary<-early_a_late_disc_collapse_summary[,c(4,1,2,3)]
early_a_late_disc_collapse_summary
# using DALY weights
early_disc_daly<-early_disc*early_daly
ter_disc_daly<-ter_disc*late_daly
early_a_late_disc_daly<-early_disc_daly+ter_disc_daly
early_disc_daly_collapse<-apply(early_disc_daly, c(1,3,4), sum)
ter_disc_daly_collapse<-apply(ter_disc_daly, c(1,3,4), sum)
early_a_late_disc_daly_collapse<-apply(early_a_late_disc_daly, c(1,3,4), sum)
early_a_late_disc_daly_collapse_t<-abind(early_a_late_disc_daly_collapse,apply(early_a_late_disc_daly_collapse, c(2,3), sum), along=1)
apply(early_a_late_disc_daly_collapse_t, 1, mean)
apply(early_a_late_disc_daly_collapse_t, 1, function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
apply(early_a_late_disc_daly_collapse_t, 1, function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
# discounted effects (eradication)
early_erad_disc<-early_erad
ter_erad_disc<-ter_erad
early_a_late_erad_disc<-early_erad+ter_erad
for (i in 2:cycle_num) {
early_erad_disc[,i,,]<-early_erad_disc[,i,,]/(1+disc_e[,i,,])^max(i-1-burn,0)
ter_erad_disc[,i,,]<-ter_erad_disc[,i,,]/(1+disc_e[,i,,])^max(i-1-burn,0)
early_a_late_erad_disc[,i,,]<-early_a_late_erad_disc[,i,,]/(1+disc_e[,i,,])^max(i-1-burn,0)
}
early_erad_disc_collapse<-apply(early_erad_disc, c(1,3,4), sum)
ter_erad_disc_collapse<-apply(early_erad_disc, c(1,3,4), sum)
early_a_late_erad_disc_collapse<-apply(early_a_late_erad_disc, c(1,3,4), sum)
early_a_late_erad_disc_collapse_t<-abind(early_a_late_erad_disc_collapse,apply(early_a_late_erad_disc_collapse, c(2,3), sum), along=1)
apply(early_a_late_erad_disc_collapse_t, 1, mean)
apply(early_a_late_erad_disc_collapse_t, 1, function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
apply(early_a_late_erad_disc_collapse_t, 1, function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
# using DALY weights
early_erad_disc_daly<-early_erad_disc*early_daly
ter_erad_disc_daly<-ter_erad_disc*late_daly
early_a_late_erad_disc_daly<-early_erad_disc_daly+ter_erad_disc_daly
early_erad_disc_daly_collapse<-apply(early_erad_disc_daly, c(1,3,4), sum)
ter_erad_disc_daly_collapse<-apply(ter_erad_disc_daly, c(1,3,4), sum)
early_a_late_erad_disc_daly_collapse<-apply(early_a_late_erad_disc_daly, c(1,3,4), sum)
early_a_late_erad_disc_daly_collapse_t<-abind(early_a_late_erad_disc_daly_collapse,apply(early_a_late_erad_disc_daly_collapse, c(2,3), sum), along=1)
apply(early_a_late_erad_disc_daly_collapse_t, 1, mean)
apply(early_a_late_erad_disc_daly_collapse_t, 1, function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
apply(early_a_late_erad_disc_daly_collapse_t, 1, function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
#########################################calculate costs#################################
# target percentage per year
tar_p<-array4dz
# for simplicity we assumed all population targeted in first period
tar_p[,burn+1,,]<-1
tar_0_4<-par_0_4*tar_p
tar_5_9<-par_5_9*tar_p
tar_10_14<-par_10_14*tar_p
tar_15_99<-par_15_99*tar_p
# contacts
# in (TTT) round(s) only
contacts<-array4dz
for (i in 1:ctry_num) {contacts[i,(burn+1+1):cycle_num,,]<-runif(ndunc,min=8, max=12)}
# actual number targeted for treatment will include "mop up" of remaining cases and their contacts in TTT only
mop<-array4dz
mop[,(burn+1+1):cycle_num,,]<-(pri_erad[,(burn+1+1):cycle_num,,]+sec_erad[,(burn+1+1):cycle_num,,])*(1+contacts[,(burn+1+1):cycle_num,,])
# we assume contacts average contact is aged 10-14
txd_0_4<-tar_0_4
txd_5_9<-tar_5_9
txd_10_14<-tar_10_14+mop
txd_15_99<-tar_15_99
# drug costs (including 10% buffer)
azi_mgs<-array4dz
azi_mgs[,,,]<-(txd_0_4[,,,]*500+txd_5_9[,,,]*1000+txd_10_14[,,,]*1500+txd_15_99[,,,]*2000)*eligible[,,,]*cover[,,,]
uc_azi<-0.17
azi_usd<-uc_azi*azi_mgs/500*1.1
# clinical misdiagnosis
misdx<-array4dz
for (i in 1:ctry_num) {misdx[i,,,]<-runif(ndunc,min=0.1, max=0.3)}
# diagnostic costs
# population to be tested serogically
sero_num<-array4dz
sero_num[,,,]<-(pri_erad+sec_erad)*(1+misdx)
uc_sero<-2
sero_usd<-uc_sero*sero_num
# implementation
# based on benchmark values
uc_mda_bmk<-uc_mda_bmk[with(uc_mda_bmk, order(country)),]
# sort by country, as in other databases
uc_mda<-array4dz
for (i in 1:ctry_num) {
temp<-exp(rnorm(ndunc,mean=uc_mda_bmk$ln_uc_econ_iu0_ac1_xb[i], sd=uc_mda_bmk$ln_uc_econ_iu0_ac1_se[i]))
for (j in 1:ndunc) {uc_mda[i,,j,]<-temp[j]}
}
# TCT
implem_num<-array4dz
implem_num[,burn+1,,]<-txd_0_4[,burn+1,,]+txd_5_9[,burn+1,,]+txd_10_14[,burn+1,,]+txd_15_99[,burn+1,,]
# TTT, if required, 30-50% of cost of TCT based on Lihir island
ttt_pc<-array4dz
for (i in 1:ctry_num) {ttt_pc[i,,,]<-runif(ndunc,min=0.3, max=0.5)}
implem_num[,burn+1+1,,]<-implem_num[,burn+1,,]*ttt_pc[,burn+1+1,,]
implem_usd<-uc_mda*implem_num
# surveillance in the subsequent period, for unknown duration
surv_years<-array4dz
for (i in 1:ctry_num) {surv_years[i,(burn+1+1+1),,]<-runif(ndunc,min=1, max=3)}
uc_surv<-array4dz
for (i in 1:ctry_num) {uc_surv[i,,,]<-runif(ndunc,min=2000/100000, max=30000/100000)}
surv_num<-array4dz
surv_num[,,,]<-(par_0_4[,,,]+par_5_9[,,,]+par_10_14[,,,]+par_15_99[,,,])*surv_years[,,,]
surv_usd<-uc_surv*surv_num
# quantities by line item
mean(apply(azi_mgs,c(3,4),sum))
mean(apply(azi_mgs,c(3,4),sum))/375000000000
quantile(apply(azi_mgs,c(3,4),sum),c(0.5,0.05,0.95))
mean(apply(sero_num,c(3,4),sum))
quantile(apply(sero_num,c(3,4),sum),c(0.5,0.05,0.95))
# cost by line item
mean(apply(azi_usd,c(3,4),sum))
quantile(apply(azi_usd,c(3,4),sum),c(0.5,0.05,0.95))
mean(apply(sero_usd,c(3,4),sum))
quantile(apply(sero_usd,c(3,4),sum),c(0.5,0.05,0.95))
mean(apply(implem_usd,c(3,4),sum))
quantile(apply(implem_usd,c(3,4),sum),c(0.5,0.05,0.95))
mean(apply(surv_usd,c(3,4),sum))
quantile(apply(surv_usd,c(3,4),sum),c(0.5,0.05,0.95))
# total cost (undiscounted)
total_usd<-azi_usd+sero_usd+implem_usd+surv_usd
mean(apply(total_usd,c(3,4),sum))
quantile(apply(total_usd,c(3,4),sum),c(0.5,0.05,0.95))
mean(apply(total_usd,c(3,4),sum))/sum(gecon_subsub$MER2005_40)
mean(apply(total_usd,c(3,4),sum))/sum(gecon_sub$MER2005_40)
total_ntx_usd<-sero_usd+implem_usd+surv_usd
mean(apply(total_ntx_usd,c(3,4),sum))
quantile(apply(total_ntx_usd,c(3,4),sum),c(0.5,0.05,0.95))
# as % of GDP
mean(apply(total_usd,c(3,4),sum)/sum(gecon_subsub$MER2005_40))
mean(apply(total_usd,c(3,4),sum)/sum(gecon_sub$MER2005_40))
# financial cost excluding drugs and diagnostic tests
fin_pc<-array4dz
for (i in 1:ctry_num) {fin_pc[i,,,]<-runif(ndunc,min=0.28, max=0.86)}
total_fin_usd<-(sero_usd+implem_usd+surv_usd)*fin_pc
mean(apply(total_fin_usd,c(3,4),sum))
quantile(apply(total_fin_usd,c(3,4),sum),c(0.5,0.05,0.95))
# surveillance in the 76 countries of unknown endemicity
surv_usd_unknown<-runif(ndunc,min=2000/100000, max=30000/100000)*par_unknown
mean(surv_usd_unknown)
quantile(surv_usd_unknown,c(0.5,0.05,0.95))
####################### discount total costs for the period#################
#discount rate on costs (country but not year specific)
temp<-array(1-exp(-runif(ndunc,min=0.03, max=0.06)*cycle_len),dim=c(ctry_num, ndunc))
disc_c<-array4d
for (i in 1:ctry_num) {
for (j in 1:ndunc){
disc_c[i,,j,]<-temp[i,j]
}
# discounted costs (eradication)
total_usd_disc<-total_usd
total_ntx_usd_disc<-total_ntx_usd
for (i in (burn+1):cycle_num) {
total_usd_disc[,i,,]<-total_usd_disc[,i,,]/(1+disc_c[,i,,])^max(i-1-burn,0)
total_ntx_usd_disc[,i,,]<-total_ntx_usd_disc[,i,,]/(1+disc_c[,i,,])^max(i-1-burn,0)
}
# summary table of costs (discounted)
total_usd_disc_collapse<-apply(total_usd_disc, c(1,3,4), sum)
total_usd_disc_t<-abind(total_usd_disc_collapse,apply(total_usd_disc_collapse, c(2,3), sum), along=1)
total_usd_disc_summary<-as.data.frame(apply(total_usd_disc_t, 1, mean))
colnames(total_usd_disc_summary)<-"best"
total_usd_disc_summary$lo<-apply(total_usd_disc_t, 1, function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
total_usd_disc_summary$hi<-apply(total_usd_disc_t, 1, function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
total_usd_disc_summary$country<-row.names(total_usd_disc_summary)
total_usd_disc_summary<-total_usd_disc_summary[,c(4,1,2,3)]
total_usd_disc_summary
# summary table of costs excluding drugs (discounted)
total_ntx_usd_disc_collapse<-apply(total_ntx_usd_disc, c(1,3,4), sum)
total_ntx_usd_disc_t<-abind(total_ntx_usd_disc_collapse,apply(total_ntx_usd_disc_collapse, c(2,3), sum), along=1)
total_ntx_usd_disc_summary<-as.data.frame(apply(total_ntx_usd_disc_t, 1, mean))
colnames(total_ntx_usd_disc_summary)<-"best"
total_ntx_usd_disc_summary$lo<-apply(total_ntx_usd_disc_t, 1, function(x) quantile(x, probs=c(0.05),na.rm=TRUE))
total_ntx_usd_disc_summary$hi<-apply(total_ntx_usd_disc_t, 1, function(x) quantile(x, probs=c(0.95),na.rm=TRUE))
total_ntx_usd_disc_summary$country<-row.names(total_ntx_usd_disc_summary)
total_ntx_usd_disc_summary<-total_ntx_usd_disc_summary[,c(4,1,2,3)]
total_ntx_usd_disc_summary
########################estimate cost-effectiveness###########################
# incremental cost effectiveness
incr_cost<-apply(total_usd_disc_collapse, c(2,3), sum)
incr_effect<-apply(early_a_late_disc_collapse,c(2,3),sum)-apply(early_a_late_erad_disc_collapse, c(2,3), sum)
icer<-incr_cost/incr_effect
mean(icer)
quantile(icer,c(0.05,0.95))
# using disability weights
incr_effect_daly<-apply(early_a_late_disc_daly_collapse,c(2,3),sum)-apply(early_a_late_erad_disc_daly_collapse,c(2,3),sum)
icer_daly<-incr_cost/incr_effect_daly
mean(icer_daly)
quantile(icer_daly,c(0.05,0.95))
######################## produce summary outputs ###########################
# cost-effectiveness acceptability curve (CEAC)
quantile(icer,c(0.50,0.90))
plot(ecdf(icer), main="", xlab="acceptability threshold (US$)", ylab="probability that cost per life-year < threshold", xlim=c(0,100), do.points = FALSE)
df <- as.data.frame(icer)
names(df)<-c("x")
ggplot(df, aes(x=x)) + stat_ecdf(size=1)  + theme_tufte() + geom_rangeframe(stat="ecdf", sides="bt") + coord_cartesian(xlim = c(0, 50)) +
geom_segment(aes(x=0,y=0.5,xend=quantile(icer,c(0.50)),yend=0.50), colour="red") +
geom_segment(aes(x=0,y=0.9,xend=quantile(icer,c(0.90)),yend=0.90), colour="red") +
geom_segment(aes(x=quantile(icer,c(0.50)),y=0,xend=quantile(icer,c(0.50)),yend=0.50), colour="red") +
geom_segment(aes(x=quantile(icer,c(0.90)),y=0,xend=quantile(icer,c(0.90)),yend=0.90), colour="red") +
scale_x_continuous("acceptability threshold (US$)") +
scale_y_continuous(breaks=c(seq(0,1,by=0.1)), "probability that cost per life-year < threshold", expand = c(0, 0))
#### optional (not verified)
# cost-effectiveness plane
scatter_y<-as.vector(incr_cost)
scatter_x<-as.vector(incr_effect)
scatter<-as.data.frame(cbind(scatter_y,scatter_x))
scatter<-transform(scatter,slope=scatter_y/scatter_x)
slope_quantiles<-quantile(scatter$slope,c(0.025,0.975))
slope_mean<-mean(scatter$slope)
p <- ggplot(scatter, aes(scatter_x/1000, scatter_y/1000))
# add titles, add threshhold based on GDP per capita (replace slope), make points slightly transparent so we can see where they overlap
r <- p + geom_point(alpha = 0.2) +
geom_abline(intercept=0, slope=slope_mean, colour = "blue")  +
geom_abline(intercept=0, slope=slope_quantiles[1], colour = "blue", linetype="dotdash")  +
geom_abline(intercept=0, slope=slope_quantiles[2], colour = "blue",  linetype="dotdash")  +
scale_x_continuous(limits=c(0,25000), "Incremental effects (thousands of life-years of yaws averted)") +
scale_y_continuous(limits=c(0,quantile(scatter_y/1000,c(0.99))), "Incremental costs (thousands US$)") + theme_tufte()
r
# expected incremental benefit
wtp<-20
eib<-(wtp*incr_effect-incr_cost)
mean(eib)
quantile(eib,c(0.025,0.975))
hist(eib/1000000, freq=FALSE)
# expected value of perfect information (EVPI)
# opportunity loss
ol<--nmb
ol[ol<0]<-0
mean(ol)
# so what if we just look at the EVPPI (perfect partial information), focussing just on the uncertainty dimension
nmb_mean<-rowMeans(nmb)
ol_part<--nmb_mean
ol_part[ol_part<0]<-0
mean(ol_part)
# or the variability dimension
nmb_mean<-colMeans(nmb)
ol_part<--nmb_mean
ol_part[ol_part<0]<-0
mean(ol_part)
# summary plots from the BCEA package (uncertainty and variability combined)
library(BCEA, lib="D:/Rlibrary")
# we first turn the 2D matrices into the BCEA vector format
fx<-cbind(as.vector(-apply(early_a_late_disc_collapse,c(2,3),sum)), as.vector(-apply(early_a_late_erad_disc_collapse,c(2,3),sum)))
# note here that the effects are negative (not QALYs)
costs<-cbind(0,as.vector(apply(total_usd_disc_collapse, c(2,3), sum)))
ints<-c("Baseline", "Eradication")
bcea<-bcea(e=fx,c=costs,ref=2,interventions=ints, Kmax=wtp)
# if matrix is too large, select 1000 rows randomly
# sample<-sample(nrow(fx),1000)
# bcea<-bcea(e=fx[sample,],c=costs[sample,],ref=1,interventions=ints, Kmax=30000)
print(bcea$ICER)
ceplane.plot(bcea,comparison=1,wtp=wtp)
eib.plot(bcea)
ceac.plot(bcea)
evi.plot(bcea)
############# TOOL BOX ##################
#
# # impose correlation structure in PSA
# # use cornode (Iman-Conover method) from mc2d to impose correlation structures
# mat <- cbind(disc_c,disc_e)
# cor(mat)
# corr1 <- matrix(c(1, 0.99, 0.99, 1), ncol=2)
# matc <- cornode(mat, target=corr1)
# cor(matc)
# # or use package copula
# install.packages("copula", lib="D:/Rlibrary", dependencies=TRUE)
# library(copula, lib="D:/Rlibrary")
# rho<-0.5 # Spearman's rho
# # For normal and T elliptical copulas, rho=sin(pi/2*tau)
# tau<-asin(rho)*2/pi # Kendall's tau
# temp <- mvdc(normalCopula(rho), c("gamma", "norm"), list(list(shape = 1, rate = 2), list(mean = 0, sd =2)))
# dat <- rMvdc(1000, temp)
# cor(dat)
# # The Frank copula is a symmetric Archimedean copula. The relationship between Kendall's tau rank correlation coefficient and the Frank copula parameter is given by (1-tau)/4
# # The Clayton copula is an asymmetric Archimedean copula, exhibiting greater dependence in the negative tail than in the positive. The relationship between Kendall's tau and the Clayton copula parameter is given by 2*tau/(1-tau)
# # The Gumbel copula (a.k.a. Gumbel-Hougard copula) is an asymmetric Archimedean copula, exhibiting greater dependence in the positive tail than in the negative. The relationship is given by 1/(1-tau)
# # http://www.vosesoftware.com/ModelRiskHelp/index.htm#Modeling_correlation/Copulas.htm
#
# # import data from WDI
# # http://data.worldbank.org/data-catalog/world-development-indicators
# # http://cran.r-project.org/web/packages/WDI/index.html
# library(WDI, lib="D:/Rlibrary")
# library(countrycode, lib="D:/Rlibrary")
# # Use the WDIsearch function to get a list of indicators
# indicatorMetaData <- WDIsearch("Population", field="name", short=FALSE)
# # Define a list of countries for which to pull data
# countries <- c("United States", "Britain", "Sweden", "Germany")
# # Convert the country names to iso2c format used in the World Bank data
# iso2cNames <- countrycode(countries, "country.name", "iso2c")
# # Pull data for individual or all countries, single or multiple indicators
# wdi_gdp2 <- WDI(country="all", indicator = "SP.POP.TOTL", start=2001, end=2050, extra=TRUE)
# wdi_gdp3 <- WDI(country="all", indicator=c("NV.AGR.PCAP.KD.ZG","GDPPCKD"), start=2001, end=2011, extra=TRUE)
# ## Filter out the aggregates
# subData <- subset(wdi_gdp3, !iso3c %in% NA)
#
# # alternative distributions
# # http://cran.r-project.org/web/packages/rriskDistributions/
# test<-rnorm(10000,100,20)
# hist(test)
# # load GUI to test all distributions
# fit.cont(test)
# # get parameters for a specific distribution by maximum likelihood
# # including: "norm", "exp", "lnorm", "logis", "gamma", "weibull", "beta", "chisq", "t", "f", "cauchy", "gompertz"
# a<-rriskFitdist.cont(test, "norm", method = c("mle"))
# a$estimate
# # get parameters for a specific distribution by method of moments
# b<-rriskFitdist.cont(test, "norm", method = c("mme"))
# b$estimate
# a<-fit.perc(p=c(0.025,0.5,0.6,0.975),q=c(1,3.5,4,7))
# a$fittedParams
# # or if you think you know the distribution
# # triangle, from three or more quantiles
# b<-get.triang.par(p=c(0.025,0.5,0.975),q=c(1,3,5))
# b
# # pert, from four or more quantiles
# c<-get.pert.par(p=c(0.025,0.5,0.6,0.975),q=c(1,3.5,4,7))
# c
# # beta, from two or more quantiles
# d<-get.beta.par(p=c(0.025,0.975),q=c(0.2,0.8))
# d
# # gamma, from two or more quantiles
# e<-get.gamma.par(p=c(0.025,0.975),q=c(1,5))
# e
# # log normal, from two or more
# f<-get.lnorm.par(p=c(0.025,0.5,0.975),q=c(0.88,1.88,4.31))
#
